{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12459366,"sourceType":"datasetVersion","datasetId":7858187}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport datetime #for data type changes\nimport matplotlib as mpl #for visualizations\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport sklearn as sk #for machine learning analysis methods\nimport random #for random gen\nprint('All done!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:54:36.610572Z","iopub.execute_input":"2025-07-23T16:54:36.611089Z","iopub.status.idle":"2025-07-23T16:54:40.946978Z","shell.execute_reply.started":"2025-07-23T16:54:36.611056Z","shell.execute_reply":"2025-07-23T16:54:40.946006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell \nInteractiveShell.ast_node_interactivity = \"all\"\n#to allow multiple outputs from one code cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:55:09.909812Z","iopub.execute_input":"2025-07-23T16:55:09.910553Z","iopub.status.idle":"2025-07-23T16:55:09.915883Z","shell.execute_reply.started":"2025-07-23T16:55:09.910516Z","shell.execute_reply":"2025-07-23T16:55:09.914950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import\n\nData connected. Now to read it into some data frame variables!","metadata":{}},{"cell_type":"code","source":"book_list=pd.read_csv(\"/kaggle/input/book-reviews/goodreads_works.csv\") #index_col='work_id' added after taking more kaggle courses, then removed because it screws everything up downstream\n#reviews=pd.read_csv(\"/kaggle/input/book-reviews/goodreads_reviews.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:55:26.609706Z","iopub.execute_input":"2025-07-23T16:55:26.610054Z","iopub.status.idle":"2025-07-23T16:55:27.248334Z","shell.execute_reply.started":"2025-07-23T16:55:26.610027Z","shell.execute_reply":"2025-07-23T16:55:27.247120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**New Error Unlocked**\n/tmp/ipykernel_36/3857527195.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  reviews=pd.read_csv(\"/kaggle/input/book-reviews/goodreads_reviews.csv\")","metadata":{}},{"cell_type":"markdown","source":"Pulling the data dictionary into a data frame to get an idea of what columns might be throwing mixed types. Also checking the stats of Kaggle's data model pages. That did not shed any light on things...","metadata":{}},{"cell_type":"code","source":"FAQ=pd.read_csv(\"/kaggle/input/book-reviews/goodreads_data_dictionary.csv\")\nFAQ\n#low_memory=False #did not help on this line\nreviews=pd.read_csv(\"/kaggle/input/book-reviews/goodreads_reviews.csv\",low_memory=False) #pandas documentation is not an easy read, so this was a natual second step\nreviews.work_id.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:55:32.530898Z","iopub.execute_input":"2025-07-23T16:55:32.531314Z","iopub.status.idle":"2025-07-23T16:56:05.021341Z","shell.execute_reply.started":"2025-07-23T16:55:32.531285Z","shell.execute_reply":"2025-07-23T16:56:05.020388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Exploration - Reviews\nFirst let's see which columns had the mixed types...","metadata":{}},{"cell_type":"code","source":"dtypes=pd.DataFrame({'review_id': reviews.review_id.dtype,'user_id': reviews.user_id.dtype,'work_id':reviews.work_id.dtype,'started_at':reviews.work_id.dtype,'read_at':reviews.work_id.dtype,'date_added':reviews.work_id.dtype,'rating':reviews.work_id.dtype,'review_text':reviews.work_id.dtype,'n_votes':reviews.work_id.dtype,'n_comments':reviews.work_id.dtype},index=[0])\ndtypes\nreviews.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:08:07.841260Z","iopub.execute_input":"2025-07-17T14:08:07.841674Z","iopub.status.idle":"2025-07-17T14:08:07.861215Z","shell.execute_reply.started":"2025-07-17T14:08:07.841646Z","shell.execute_reply":"2025-07-17T14:08:07.860318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Well, I am going to guess that the three columns being the issue are 'started_at', 'read_at' and 'date_added' because I would think they should be date/time fields, yet they are reporting as int.\n\n***Oh my god! I went back to a different lesson in Pandas and found the simple way to get the types of the columns.***\n😫😲 Now I am embarrassed, but *silver lining* I practiced creating data frames and now I will never forget this command exists! I came up here first because I just copied and pasted the code from the lesson (reviews.dtypes) so I didn't know it existed before going and working on the book_list table.\n\nNext, I am going to look at value counts per column and nulls.","metadata":{}},{"cell_type":"code","source":"reviews.review_id.value_counts()\nreviews.review_id.isnull().sum()\nreviews.user_id.value_counts()\nreviews.user_id.isnull().sum()\nreviews.work_id.value_counts()\nreviews.work_id.isnull().sum()\nreviews.started_at.value_counts()\nreviews.started_at.isnull().sum()\nreviews.read_at.value_counts()\nreviews.read_at.isnull().sum()\nreviews.date_added.value_counts()\nreviews.date_added.isnull().sum()\nreviews.rating.value_counts()\nreviews.rating.isnull().sum()\nreviews.review_text.value_counts()\nreviews.review_text.isnull().sum()\nreviews.n_votes.value_counts()\nreviews.n_votes.isnull().sum()\nreviews.n_comments.value_counts()\nreviews.n_comments.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:25:56.203998Z","iopub.execute_input":"2025-07-16T19:25:56.204407Z","iopub.status.idle":"2025-07-16T19:26:00.455330Z","shell.execute_reply.started":"2025-07-16T19:25:56.204384Z","shell.execute_reply":"2025-07-16T19:26:00.454508Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* review_id: all values unique, no nulls\n* user_id: 18800 unique values, no nulls\n* work_id: 13525 unique values, no nulls\n* started_at: 3797 unique values, 347495 nulls\n* read_at: 5868 unique values, 112122 nulls\n* date_added: 3919 unique values, no nulls\n* rating: 5 unique values, 37318 nulls\n* review_text:1130794 unique values, no nulls\n* n_votes: 605 unique values, no nulls\n* n_comments: 228 unique values, no nulls","metadata":{}},{"cell_type":"markdown","source":"I wonder if the null ratings have reviews. Perhaps I can impute a rating based on the review. There are 37318 though, so if I choose to impute, I'd need to use a function that has a reliable filter, e.g., if review contains '!' & 'good' = 5.0, if review contains 'good' = 4.0, etc.","metadata":{}},{"cell_type":"code","source":"reviews.loc[reviews.rating.isnull()==True]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:26:00.456666Z","iopub.execute_input":"2025-07-16T19:26:00.457025Z","iopub.status.idle":"2025-07-16T19:26:00.482008Z","shell.execute_reply.started":"2025-07-16T19:26:00.456994Z","shell.execute_reply":"2025-07-16T19:26:00.481132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on these results, there doesn't seem to be a reliable way to impute an accurate rating. Also, some reviews aren't indicative of the essence of the book. ","metadata":{}},{"cell_type":"markdown","source":"Checking some of the quantifiable options...","metadata":{}},{"cell_type":"code","source":"round(reviews.describe(),2)\nreviews.n_votes.max()\nreviews.n_votes.min()\nreviews.n_votes.mean()\nreviews.n_votes.median()\nreviews.rating.mean()\n37318/1143887 #percent of ratings with null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:26:00.483247Z","iopub.execute_input":"2025-07-16T19:26:00.483579Z","iopub.status.idle":"2025-07-16T19:26:00.664376Z","shell.execute_reply.started":"2025-07-16T19:26:00.483550Z","shell.execute_reply":"2025-07-16T19:26:00.663619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"n_votes may or may not be useful as it's heavily skewed...may give less weight if used","metadata":{}},{"cell_type":"markdown","source":"## Data Exploration - Works","metadata":{}},{"cell_type":"code","source":"book_list.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T15:44:36.144434Z","iopub.execute_input":"2025-07-17T15:44:36.144752Z","iopub.status.idle":"2025-07-17T15:44:36.162465Z","shell.execute_reply.started":"2025-07-17T15:44:36.144728Z","shell.execute_reply":"2025-07-17T15:44:36.161689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Well, I don't think I need to keep image_url for my purposes. I also want to get a count on nulls in isbn and isbn13 because that data should be available, but the work involved if it's alot...\nKaggle didn't throw a data type error for this table, but I want to confirm what the are.","metadata":{}},{"cell_type":"code","source":"#book_list.describe()\ndata_types=pd.DataFrame({'work_id':book_list.work_id.dtype,'isbn':book_list.isbn.dtype,'isbn13':book_list.isbn13.dtype,'orig_pub_yr':book_list.original_publication_year.dtype,'num_pages':book_list.num_pages.dtype,'description':book_list.description.dtype,'genres':book_list.genres.dtype,'num_reviews':book_list.reviews_count.dtype,'num_text_reviews':book_list.text_reviews_count.dtype,'num_5stars':book_list['5_star_ratings'].dtype,'num_4stars':book_list['4_star_ratings'].dtype,'num_3stars':book_list['3_star_ratings'].dtype,'num_2stars':book_list['2_star_ratings'].dtype,'num_1stars':book_list['1_star_ratings'].dtype, 'num_ratings': book_list.ratings_count.dtype,'avg_rating':book_list.avg_rating.dtype,'similar_books':book_list.similar_books.dtype},index=[0])\ndata_types\n#book_list.5_star_ratings #invalid\n#book_list['5_star_ratings'] #valid\n#book_list.description[1]\nbook_list.dtypes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:41:28.160781Z","iopub.execute_input":"2025-07-17T13:41:28.161086Z","iopub.status.idle":"2025-07-17T13:41:28.171965Z","shell.execute_reply.started":"2025-07-17T13:41:28.161063Z","shell.execute_reply":"2025-07-17T13:41:28.171088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"That was fun figuring out the correct way to select a column that starts with a digit!\nAlso, I wish I knew about how you can call dtypes on the data frame before...it's nice not having to horizontally scroll. \nType recommendations have been recorded and now we move on the the null testing...","metadata":{}},{"cell_type":"markdown","source":"I added in this code section to check for duplicates in the key field because \"remove duplicates was part of my project\" and I wanted to make sure...we know what happens when you assume.","metadata":{}},{"cell_type":"code","source":"book_list.work_id.value_counts()\nbook_list.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:26:00.728700Z","iopub.execute_input":"2025-07-16T19:26:00.729014Z","iopub.status.idle":"2025-07-16T19:26:00.750235Z","shell.execute_reply.started":"2025-07-16T19:26:00.728991Z","shell.execute_reply":"2025-07-16T19:26:00.749131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"book_list.work_id.isnull().sum()\nbook_list.isbn.isnull().sum()\nbook_list.isbn13.isnull().sum()\nbook_list.original_publication_year.isnull().sum()\nbook_list.num_pages.isnull().sum()\nbook_list.description.isnull().sum()\nbook_list.genres.isnull().sum()\nbook_list.reviews_count.isnull().sum()\nbook_list.text_reviews_count.isnull().sum()\nbook_list['5_star_ratings'].isnull().sum()\nbook_list['4_star_ratings'].isnull().sum()\nbook_list['3_star_ratings'].isnull().sum()\nbook_list['2_star_ratings'].isnull().sum()\nbook_list['1_star_ratings'].isnull().sum()\nbook_list.ratings_count.isnull().sum()\nbook_list.avg_rating.isnull().sum()\nbook_list.similar_books.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:26:00.751178Z","iopub.execute_input":"2025-07-16T19:26:00.751437Z","iopub.status.idle":"2025-07-16T19:26:00.825035Z","shell.execute_reply.started":"2025-07-16T19:26:00.751417Z","shell.execute_reply":"2025-07-16T19:26:00.824028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I find it odd that there are so many nulls in isbn and isbn13. ISBNs are to books as SSNs are to natural-born US citizens. Right now it is not worth it to look up all those missing values.\nI can find 18 publication years...that could be useful when curating a reading list. Number of pages would likewise be useful, but right now 730 missing values is not worth figuring out how to find.","metadata":{}},{"cell_type":"markdown","source":"Looking deeper into the anomalies of num_pages...\n* statistics are weird\n* low page books aren't unusual, but after checking a few, they are more than likely incorrect","metadata":{}},{"cell_type":"code","source":"book_list.num_pages.describe()\nbook_list.loc[book_list.num_pages.isnull()==True]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:26:00.825894Z","iopub.execute_input":"2025-07-16T19:26:00.826138Z","iopub.status.idle":"2025-07-16T19:26:00.856036Z","shell.execute_reply.started":"2025-07-16T19:26:00.826119Z","shell.execute_reply":"2025-07-16T19:26:00.854957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Findings and Action Items\n### Reviews Table\n* Data type of started_at, read_at and date_added should be changed from int64 to date/time.\n* Data type of review_id, user_id and review_text should be changed from object to string.\n* Significant amount of nulls in started_at, read_at and rating. Recommend eliminating started_at and read_at due to lack of relevance to book recommendation.\n* 3% of rows in rating are null. Propose two options:\n    * Impute the average rating value of 3.8\n    * Replace with N/A or None (eliminates mathematical operations for analysis)\n* Recommend putting less weight on using rating as an input for recommendation or having a null rating handler function\n\n### Book_List Table\n* isbn and isbn13 should be same type, can change to int or string\n* original_publish_year can be changed from float to date/time\n* num_pages should be changed to int\n* Description can be changed to string\n    * Consider removing the \\n coding during transformation\n* Recommendations for nulls\n    * isbn and isbn13 replace with Missing if a test shows that 1% do exist but aren't listed, replace with None or Unknown if 1% cannot be found\n    * Research original publish years and add values\n    * descriptions replace with Missing\n    * num_pages consider how the data will be used in recommendations...using non-numeric replacement disallows mathematical functions, using 0 will skew mathematical results, mean or median may be appropriate\n* Remove image url column ","metadata":{}},{"cell_type":"markdown","source":"# Transformation\n","metadata":{}},{"cell_type":"markdown","source":"## Null Handling\nRecommendations for nulls\n* isbn and isbn13 replace with Missing if a test shows that 1% do exist but aren't listed, replace with None or Unknown if 1% cannot be found\n* Research original publish years and add values\n* descriptions replace with Missing\n* num_pages consider how the data will be used in recommendations...using non-numeric replacement disallows mathematical functions, using 0 will skew mathematical results, mean or median may be appropriate\n\n* Significant amount of nulls in started_at, read_at and rating.\n    * Recommend eliminating started_at and read_at due to lack of relevance to book recommendation.\n* 3% of rows in rating are null. Propose two options:\n    * Impute the average rating value of 3.8\n    * Replace with N/A or None (eliminates mathematical operations for analysis)","metadata":{}},{"cell_type":"code","source":"reviews.rating.median()\nreviews.rating.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:26:00.857234Z","iopub.execute_input":"2025-07-16T19:26:00.857512Z","iopub.status.idle":"2025-07-16T19:26:00.880239Z","shell.execute_reply.started":"2025-07-16T19:26:00.857492Z","shell.execute_reply":"2025-07-16T19:26:00.879343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"book_list['description']=book_list.description.fillna('Missing')\nbook_list['num_pages']=book_list.num_pages.fillna(0)\nbook_list['num_pages']=book_list.num_pages.fillna(0) #after discussing with my wingman\nbook_list.head()\nreviews['rating']=reviews.rating.fillna(3.8)\nreviews.rating.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:56:05.069218Z","iopub.execute_input":"2025-07-23T16:56:05.069449Z","iopub.status.idle":"2025-07-23T16:56:05.113529Z","shell.execute_reply.started":"2025-07-23T16:56:05.069431Z","shell.execute_reply":"2025-07-23T16:56:05.112505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A-reasearching we go! First up are publication years...","metadata":{}},{"cell_type":"code","source":"#book_list.loc[book_list.original_publication_year.isnull()==True]\n#book_list.original_publication_year[52]=2016\nbook_list.loc[52,'original_publication_year']=2016\nbook_list.loc[52].original_publication_year\nbook_list.loc[239].original_publication_year #result: nan\nbook_list.loc[239,'original_publication_year']=2007\nbook_list.loc[239].original_publication_year #result: 2007 🎉Yay! I'm changing what I want changed!!\nbook_list.loc[291,'original_publication_year']=2008\nbook_list.loc[445,'original_publication_year']=2006\nbook_list.loc[2638,'original_publication_year']=2012\nbook_list.loc[3105,'original_publication_year']=2012\nbook_list.loc[3152,'original_publication_year']=2015\nbook_list.loc[4284,'original_publication_year']=2014\nbook_list.loc[10630,'original_publication_year']=2016\nbook_list.loc[11195,'original_publication_year']=2016\nbook_list.loc[11219,'original_publication_year']=2001\nbook_list.loc[11237,'original_publication_year']=2016\nbook_list.loc[11700,'original_publication_year']=2016\nbook_list.loc[12111,'original_publication_year']=2016\nbook_list.loc[12118,'original_publication_year']=2013\nbook_list.loc[12145,'original_publication_year']=2016\nbook_list.loc[12195,'original_publication_year']=2016\nbook_list.loc[13235,'original_publication_year']=2016\n#book_list.loc[12235]\n#book_list.original_publication_year.isnull().sum() #whoops! I had a typo and changed 12235 by mistake to 2016...better check what it should be\n#book_list.loc[12235,'original_publication_year']=2011 #crisis averted\n#book_list.loc[12235]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:56:15.836981Z","iopub.execute_input":"2025-07-23T16:56:15.837381Z","iopub.status.idle":"2025-07-23T16:56:15.864632Z","shell.execute_reply.started":"2025-07-23T16:56:15.837354Z","shell.execute_reply":"2025-07-23T16:56:15.863801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Keeping an eye on those last five with missing isbn/isbn13, only one did not have any...it had an ASIN instead\nReplace isbn and isbn13 NaN with \"Missing\"","metadata":{}},{"cell_type":"code","source":"book_list['isbn13']=pd.to_numeric(book_list['isbn13'],errors='coerce').fillna(0).astype('int64').astype('string')#guidance from ChatGPT to force isbn13 to behave\nbook_list.isbn13.head()\nbook_list[['isbn','isbn13']]=book_list[['isbn','isbn13']].fillna('Missing')\nbook_list.isbn.head()\nbook_list['isbn13']=book_list['isbn13'].replace('0','Missing')\nbook_list.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:56:25.357781Z","iopub.execute_input":"2025-07-23T16:56:25.359509Z","iopub.status.idle":"2025-07-23T16:56:25.421432Z","shell.execute_reply.started":"2025-07-23T16:56:25.359456Z","shell.execute_reply":"2025-07-23T16:56:25.420510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"[consulting with ChatGPT about how to update nulls in a column *in situ*](https://chatgpt.com/share/6876aabe-ff4c-8005-b99c-cf045a8c952c)","metadata":{}},{"cell_type":"markdown","source":"## Data Types","metadata":{}},{"cell_type":"markdown","source":"* Data type of started_at, read_at and date_added should be changed from int64 to date/time.\n* Data type of review_id, user_id and review_text should be changed from object to string.\n* isbn and isbn13 should be same type, can change to int or string\n* original_publish_year can be changed from float to date/time\n* num_pages should be changed to int\n* Description can be changed to string\n    * Consider removing the \\n coding during transformation","metadata":{}},{"cell_type":"code","source":"reviews.review_id.astype(\"string\")\nreviews.review_text.astype(\"string\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:56:49.130568Z","iopub.execute_input":"2025-07-23T16:56:49.131738Z","iopub.status.idle":"2025-07-23T16:56:49.288517Z","shell.execute_reply.started":"2025-07-23T16:56:49.131692Z","shell.execute_reply":"2025-07-23T16:56:49.287416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\"One peculiarity to keep in mind (and on display very clearly here) is that columns consisting entirely of strings do not get their own type; they are instead given the object type.\"\n🤦🏼‍♀️Read Mel","metadata":{}},{"cell_type":"code","source":"#book_list.isbn.astype(int) #ValueError: cannot convert float NaN to integer\n#book_list.isbn.astype(float) #ValueError: could not convert string to float: '080075963X' Fun fact-ISBN check digits are base eleven so they include digits 0-9 and 'X'\n#book_list.isbn13.astype(float)\nbook_list.isbn.astype(\"string\")\n#for consistency's sake and because there will be no need for math on ISBN13, let's make it a string too\nbook_list.isbn13.astype(\"string\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:57:00.235966Z","iopub.execute_input":"2025-07-23T16:57:00.236372Z","iopub.status.idle":"2025-07-23T16:57:00.267324Z","shell.execute_reply.started":"2025-07-23T16:57:00.236342Z","shell.execute_reply":"2025-07-23T16:57:00.266247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#book_list.num_pages.astype(int) #IntCastingNaNError: Cannot convert non-finite values (NA or inf) to integer\n#need to decide what to do about the nulls...for now change them all to 0 because there are no 0 entries > 1 was the min\n#book_list.loc[book_list.num_pages.isnull()] before ChatGPT discussion\nbook_list['num_pages']=book_list.num_pages.astype(int)\nbook_list['original_publication_year']=book_list.original_publication_year.astype(int)\nbook_list.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:57:03.129603Z","iopub.execute_input":"2025-07-23T16:57:03.129931Z","iopub.status.idle":"2025-07-23T16:57:03.139647Z","shell.execute_reply.started":"2025-07-23T16:57:03.129907Z","shell.execute_reply":"2025-07-23T16:57:03.138849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Datetime experimentation...","metadata":{}},{"cell_type":"code","source":"#temp_year=pd.DataFrame({'Year':book_list.original_publication_year.copy()})\n#temp_year.Year.astype(str)\n#pd.to_datetime('Year',yearfirst=True,format=\"mixed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:26:03.083273Z","iopub.status.idle":"2025-07-16T19:26:03.083521Z","shell.execute_reply.started":"2025-07-16T19:26:03.083407Z","shell.execute_reply":"2025-07-16T19:26:03.083418Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are throwing a fit because we do not recognize a year on its own to parse as a date. There isn't a good reason except for \"but it's time!\" to convert it from an int, so I'm crawling out of this rabbit hole.","metadata":{}},{"cell_type":"markdown","source":"## Standardization\nInspect what you expect and expect what you inspect","metadata":{}},{"cell_type":"code","source":"#reviews.head()\n#reviews.loc[reviews.started_at.notnull()]\nreviews=reviews.rename(columns={'started_at': 'started','read_at':'finished','date_added':'review_added','review_text': 'review','n_votes': 'num_votes','n_comments':'num_comments'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:57:26.344706Z","iopub.execute_input":"2025-07-23T16:57:26.345603Z","iopub.status.idle":"2025-07-23T16:57:26.662814Z","shell.execute_reply.started":"2025-07-23T16:57:26.345568Z","shell.execute_reply":"2025-07-23T16:57:26.661917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"book_list.head()\nbook_list=book_list.drop('image_url',axis=1)\nreviews=reviews.drop(['started','finished'],axis=1)\n\nbook_list=book_list.rename(columns={'original_title':'title','reviews_count':'total_reviews','text_reviews_count':'num_written_reviews','5_star_ratings':'5stars','4_star_ratings':'4stars','3_star_ratings':'3stars','2_star_ratings':'2stars','1_star_ratings':'1star','ratings_count':'total_ratings'})\nbook_list.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:57:33.989910Z","iopub.execute_input":"2025-07-23T16:57:33.990294Z","iopub.status.idle":"2025-07-23T16:57:34.194914Z","shell.execute_reply.started":"2025-07-23T16:57:33.990270Z","shell.execute_reply":"2025-07-23T16:57:34.193852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normalization","metadata":{}},{"cell_type":"code","source":"#temp=book_list.copy() before I break anything, let's experiment on a copy\nbook_list['description']=book_list['description'].str.strip()\nbook_list['title']=book_list['title'].str.strip()\nbook_list['author']=book_list['author'].str.strip()\nbook_list['genres']=book_list['genres'].str.strip()\nbook_list.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:57:45.090486Z","iopub.execute_input":"2025-07-23T16:57:45.090778Z","iopub.status.idle":"2025-07-23T16:57:45.123352Z","shell.execute_reply.started":"2025-07-23T16:57:45.090756Z","shell.execute_reply":"2025-07-23T16:57:45.122476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Using a temp df again to experiment with 'extract' to get the \".0\" off the end of isbn13","metadata":{}},{"cell_type":"code","source":"#temp=book_list.copy()\n#temp['isbn13']=temp.isbn13.replace('Missing',0)\n#temp.isbn13.astype('int64') #python is refusing to convert the ISBN13 to a string, it only treats it as a float\n\n#temp.isbn13.str.extract(r\"(['.0'])\")\n#temp.isbn13.str.rsplit('.',expand=True)\n#temp['isbn13']=temp['isbn13'].str[:-2]\n#temp.loc[2]\n#temp['isbn13']=temp['isbn13'].str.rstrip()\n#temp.loc[2]\n#temp['isbn13'].apply(str).value_counts()\n#temp['isbn13'].apply(lambda x:type(x)).value_counts()\n#temp.isbn13.dtype\n#temp['isbn13'].apply(lambda x:str(x))\n#temp['isbn13'].apply(lambda x:f\"{x:.0f})\n#temp['isbn13']=temp.isbn13.replace(0,'Missing') #does not replace\n\n#print(temp['isbn13'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:58:35.435341Z","iopub.execute_input":"2025-07-16T19:58:35.435644Z","iopub.status.idle":"2025-07-16T19:58:35.443137Z","shell.execute_reply.started":"2025-07-16T19:58:35.435621Z","shell.execute_reply":"2025-07-16T19:58:35.441891Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**WTF Mate?**\nIn the code cell above, I was having issues. Looking back, I think I screwed everything up by converting the float directly to string without the int intermediary. The data was carrying a '.0' that it refused to get rid of despite all my efforts. I researched the internet on my own and even created a [separate testing notebook](https://www.kaggle.com/code/melisandefritzsche/testing) to use for testing concepts and troubleshooting. At a loss, I consulted with my AI partner.\n[ChatGPT consultation](https://chatgpt.com/share/6879014b-ffe0-8005-9a77-cf04c80b590f)\n*Solution*\nI implemented the solution up in the [null handling](https://www.kaggle.com/code/melisandefritzsche/bookshelf-challenge/edit/run/250847672#Null-Handling) section.","metadata":{}},{"cell_type":"code","source":"book_list=book_list.set_index('work_id')\nreviews=reviews.set_index('review_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:57:54.729533Z","iopub.execute_input":"2025-07-23T16:57:54.729846Z","iopub.status.idle":"2025-07-23T16:57:54.870173Z","shell.execute_reply.started":"2025-07-23T16:57:54.729824Z","shell.execute_reply":"2025-07-23T16:57:54.869017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting and Extracting","metadata":{}},{"cell_type":"markdown","source":"separating the genre lists into individual genres will assist in grouping","metadata":{}},{"cell_type":"code","source":"genres=book_list.copy()\ngenres.shape\nbook_list.head()\n#genre_split=genres['genres'].str.split(',',expand=True)\n#genre_split\n#genre_split=genres['genres'].str.split(',').explode('genres').reset_index                                                \n#genre_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:58:00.729791Z","iopub.execute_input":"2025-07-23T16:58:00.730059Z","iopub.status.idle":"2025-07-23T16:58:00.754692Z","shell.execute_reply.started":"2025-07-23T16:58:00.730040Z","shell.execute_reply":"2025-07-23T16:58:00.753741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I am going to try to insert the unexpanded list version of the string into the column using replace and then explode it","metadata":{}},{"cell_type":"code","source":"genres['genres']=genres['genres'].str.split(',')\ngenres.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:58:15.674986Z","iopub.execute_input":"2025-07-23T16:58:15.675364Z","iopub.status.idle":"2025-07-23T16:58:15.711083Z","shell.execute_reply.started":"2025-07-23T16:58:15.675340Z","shell.execute_reply":"2025-07-23T16:58:15.710320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"genres_spl=genres.explode('genres')\ngenres_spl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:58:18.420099Z","iopub.execute_input":"2025-07-23T16:58:18.421355Z","iopub.status.idle":"2025-07-23T16:58:18.496852Z","shell.execute_reply.started":"2025-07-23T16:58:18.421294Z","shell.execute_reply":"2025-07-23T16:58:18.495536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"🤩**I WIN!**","metadata":{}},{"cell_type":"code","source":"genres_spl.genres.value_counts()#checking for duplicates\ngenres_spl['genres']=genres_spl.genres.str.strip() #previously did not have the genres_spl['genres']=...learning curve observation-I need to get used to syntax for Python just doing a thing versus Pythong doing the thing and keeping it done\n#genres_spl.reindex()\n#genres_spl.head()\ngenres_spl.groupby('genres').count().sort_values(by='title',ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:58:21.679865Z","iopub.execute_input":"2025-07-23T16:58:21.680297Z","iopub.status.idle":"2025-07-23T16:58:21.790411Z","shell.execute_reply.started":"2025-07-23T16:58:21.680270Z","shell.execute_reply":"2025-07-23T16:58:21.789091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#genres_spl=genres_spl.set_index('work_id') #after adding the index set to book_list, this is unneccesary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T14:18:39.337010Z","iopub.execute_input":"2025-07-22T14:18:39.337502Z","iopub.status.idle":"2025-07-22T14:18:39.356723Z","shell.execute_reply.started":"2025-07-22T14:18:39.337472Z","shell.execute_reply":"2025-07-22T14:18:39.355464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Just in case for filtering (and for coding practice) I am going to split the author field into first and last name as well.","metadata":{}},{"cell_type":"code","source":"temp=book_list.copy()\ntemp1=temp.author.str.rsplit(' ',n=1,expand=True) #I did something last time -I accidentally undid all my previous work- and inserted the new columns into the original list so I am being redundant as a form of caution\ntemp1.head()\ntemp.head()\nauthor_spl=temp\nauthor_spl.insert(4,'first_name',temp1[0])\nauthor_spl.insert(5,'last_name',temp1[1])\nauthor_spl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:58:41.671072Z","iopub.execute_input":"2025-07-23T16:58:41.671496Z","iopub.status.idle":"2025-07-23T16:58:41.730834Z","shell.execute_reply.started":"2025-07-23T16:58:41.671469Z","shell.execute_reply":"2025-07-23T16:58:41.730087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#author_spl.dtypes\n#author_spl.original_publication_year.astype(int)\nauthor_spl=author_spl.rename(columns={'original_publication_year':'year'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:59:07.909761Z","iopub.execute_input":"2025-07-23T16:59:07.910053Z","iopub.status.idle":"2025-07-23T16:59:07.921269Z","shell.execute_reply.started":"2025-07-23T16:59:07.910030Z","shell.execute_reply":"2025-07-23T16:59:07.920246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#author_pl=author_spl.set_index('work_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:10:55.180789Z","iopub.execute_input":"2025-07-21T19:10:55.181211Z","iopub.status.idle":"2025-07-21T19:10:55.199360Z","shell.execute_reply.started":"2025-07-21T19:10:55.181155Z","shell.execute_reply":"2025-07-21T19:10:55.198141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Merging and Joining","metadata":{}},{"cell_type":"code","source":"#book_list=book_list.set_index('work_id')\n#reviews=reviews.set_index('work_id')\n\n#reviews.groupby('work_id').count()\n#all_data.head()\n#reviews.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:14:36.089925Z","iopub.execute_input":"2025-07-23T17:14:36.090337Z","iopub.status.idle":"2025-07-23T17:14:36.445167Z","shell.execute_reply.started":"2025-07-23T17:14:36.090313Z","shell.execute_reply":"2025-07-23T17:14:36.444282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Line in the Sand*\nI fully admit that this is not the most efficient way to do things, but my lack of experience with Python and non-math-related analysis goals (I'm curating a book list so to my mind that basically means I'm going to be filtering and sorting. As opposed to giving statistical descriptions or predicting stuff.)\nAnyway, I am stopping here with five total data frames to use.\n1. all_data - everything joined together\n2. book_list - just the book information\n3. reviews - just the review information\n4. genres_spl - table with all genres split out\n5. author_spl - table with authors' first and last names split","metadata":{}},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"markdown","source":"## Functions\n\nThis whole \"build a tool\" thing has me a bit confused. I am thinking that I will have a Q&A (not NLP) where I obtain information from the user and then spit out their result. Therefore, I will need helper function to take particular actions like filtering. I should probably define the functionality so that my scope is clear.\n\n* Get how many books the user wants on the list per this query.\n* \"Surprise Me!\": random number generator to pick books by their work_id\n* Get preferences - fiction/non-fiction, author, genre\n* Filters","metadata":{}},{"cell_type":"code","source":"def get_num_books():\n    '''\n    Function prints a question requesting a total number of books to be curated.\n    Returns an integer value less than 13,524.\n    '''\n    print('How many books would you like on your reading list?')\n    num_books=input('Enter a number less than 13,525:')\n    if 0 < num_books < 13525:\n        return num_books\n    else:\n        print(\"Invalid entry\")\nget_num_books()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T14:23:34.794372Z","iopub.execute_input":"2025-07-22T14:23:34.794667Z","iopub.status.idle":"2025-07-22T14:23:38.657612Z","shell.execute_reply.started":"2025-07-22T14:23:34.794644Z","shell.execute_reply":"2025-07-22T14:23:38.656801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def surprise_me(int num_books):\n    '''\n    Function expects integer value for book list amount.\n    Returns a random sample of k amount from dataframe rows range. List is meant to be used to locate rows in full book_list dataframe.\n    '''\n    reading_list=random.sample(range(0,13525),k=(num_books))\n    return reading_list\n\nsurprise_me(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T14:23:50.514326Z","iopub.execute_input":"2025-07-22T14:23:50.514602Z","iopub.status.idle":"2025-07-22T14:23:50.521723Z","shell.execute_reply.started":"2025-07-22T14:23:50.514586Z","shell.execute_reply":"2025-07-22T14:23:50.520922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"books=surprise_me(3)\ntemp=book_list.sort_values(by='total_reviews',ascending=False)\nprint('SURPRISE!! Happy reading!')\ntemp.iloc[books,lambda temp:[2,3,6,7]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T14:23:54.839810Z","iopub.execute_input":"2025-07-22T14:23:54.840893Z","iopub.status.idle":"2025-07-22T14:23:54.862812Z","shell.execute_reply.started":"2025-07-22T14:23:54.840842Z","shell.execute_reply":"2025-07-22T14:23:54.861847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Take *that*, Python! 😤","metadata":{}},{"cell_type":"code","source":"book_list.sort_values(by='total_reviews',ascending=False) #This is how I'd sort my temp list if it were cleaned up and everything. Every time I go back and rerun my code I wind up breaking something so...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:08:24.426188Z","iopub.execute_input":"2025-07-21T16:08:24.426655Z","iopub.status.idle":"2025-07-21T16:08:24.458890Z","shell.execute_reply.started":"2025-07-21T16:08:24.426617Z","shell.execute_reply":"2025-07-21T16:08:24.456811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sort():\n    '''\n    Function asks for user sorting inputs, currently author's name and genre.\n    Returns rows meeting both criteria.\n    '''\n    author=input(\"Favorite author?\")\n    genre=input(\"Favorite genre?\")\n    \n    return book_list.loc[(book_list.author == author) & (book_list['genres'].str.contains(genre))]\n\nget_sort()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:28:37.694647Z","iopub.execute_input":"2025-07-23T17:28:37.694954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization in Python\nExploring what matplotlib has to offer. Also, instead of stumbling about, I took the Kaggle Data Visualization course and learned about seaborn.","metadata":{}},{"cell_type":"code","source":"genres_spl.sort_values(by='total_ratings',ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T14:32:56.874319Z","iopub.execute_input":"2025-07-22T14:32:56.874584Z","iopub.status.idle":"2025-07-22T14:32:56.932377Z","shell.execute_reply.started":"2025-07-22T14:32:56.874567Z","shell.execute_reply":"2025-07-22T14:32:56.931222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(x='avg_rating',y='total_reviews',data=genres_spl,hue='genres')\nplt.title('Average Rating by Genre')\nplt.xlabel('Avg Rating')\nplt.ylabel('Total Reviews')\nplt.legend(loc='upper right',bbox_to_anchor=(1.35,1))\n#researched the interwebs and copied but edited\ndef M_formatter(x, pos):\n    \"\"\"The function formats the tick labels to display in millions.\"\"\"\n    return f'{x / 1_000_000:.1f}M'\nplt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(M_formatter))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T14:57:53.135580Z","iopub.execute_input":"2025-07-22T14:57:53.135988Z","iopub.status.idle":"2025-07-22T14:57:56.422102Z","shell.execute_reply.started":"2025-07-22T14:57:53.135965Z","shell.execute_reply":"2025-07-22T14:57:56.421075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Huh. Looks like romance and crime/thriller get reviewed the most and get reviewed highly on average. Young adult and fiction are highest rated even with less overall reviews. That tracks.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x='avg_rating',y='ratings_count',data=genres_spl,hue='genres')\nplt.title('Average Rating by Genre')\nplt.xlabel('Avg Rating')\nplt.ylabel('Total Reviews')\nplt.legend(loc='upper right',bbox_to_anchor=(1.35,1))\n#researched the interwebs and copied but edited\ndef M_formatter(x, pos):\n    \"\"\"The function formats the tick labels to display in millions.\"\"\"\n    return f'{x / 1_000_000:.1f}M'\nplt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(M_formatter))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T16:42:15.373735Z","iopub.execute_input":"2025-07-22T16:42:15.374238Z","iopub.status.idle":"2025-07-22T16:42:15.450559Z","shell.execute_reply.started":"2025-07-22T16:42:15.374211Z","shell.execute_reply":"2025-07-22T16:42:15.449456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.barplot(x=book_list.avg_rating,y=book_list.num_pages)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T15:53:37.269716Z","iopub.execute_input":"2025-07-22T15:53:37.270054Z","iopub.status.idle":"2025-07-22T15:53:38.096230Z","shell.execute_reply.started":"2025-07-22T15:53:37.270032Z","shell.execute_reply":"2025-07-22T15:53:38.095240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.histplot(x=book_list.avg_rating,y=book_list.num_pages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T15:53:53.815708Z","iopub.execute_input":"2025-07-22T15:53:53.816724Z","iopub.status.idle":"2025-07-22T15:53:54.002837Z","shell.execute_reply.started":"2025-07-22T15:53:53.816679Z","shell.execute_reply":"2025-07-22T15:53:54.001938Z"}},"outputs":[],"execution_count":null}]}